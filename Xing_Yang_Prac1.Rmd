---
title: "Fall 2017 DA5030 Practicum 1"
author: "Xing Yang"
date: "2017/10/2"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Problem 1
###Question 1&2

```{r}
library(class)
library(caret)
library(tidyverse)
library(gmodels)
library(TTR)
data = read.table(file="/Users/xingyang/Desktop/5030/practp1.txt", sep = ",")
colnames(data) <- c("id","ri","na","mg","al","si","k","ca","ba","fe","type")
head(data)
```

###Question 3

```{r}
mean = mean(data$na)
sd = sd(data$na)
hist(data$na, freq = FALSE) 
curve(dnorm(x, mean=mean, sd=sd),col=2,add=T)
```

It is a non-parametric method.

###Question 4

```{r}
data <- data[-1]
data_n1 <- data
for(i in 1:2)
{
  min = min(data_n1[,i])
  max = max(data_n1[,i])
  data_n1[,i] = (data_n1[,i] - min) / (max - min)
}
head(data_n1)
```

###Question 5

```{r}
n = ncol(data)
data_n <- data_n1
for(i in 3:(n-1))
{
  mean = mean(data_n[,i])
  sd = sd(data_n[,i])
  data_n[,i] = (data_n[,i]-mean) / sd
}
head(data_n)
```

###Question 6

```{r}
set.seed(3000)
indxTrain <- createDataPartition(y = data_n1$na,p = 0.5,list = FALSE)
training <- data[indxTrain,]
testing <- data[-indxTrain,]
```

###Question 7

```{r}
#distance function
dist <- function(p,q)
{
 d <- 0
 for(i in 1:length(p))
 {
   d <- d + (p[i]-q[i])^2
 }
 dist <- sqrt(d)
}
neighbors <- function(data, x)
{
  m = nrow(data)
  ds <- numeric(m)
  for(i in 1:m)
  {
    p <- data[i, 1:(n-1)]
    ds[i] <- dist(p, x)
  }
  neighbors <- ds
}
kclosest <- function(neighbors, k)
{
  o <- order(neighbors)
  kclosest <- o[1:k]
}
mode <- function(x)
{
  ux <- unique(x)
  ux[which.max(tabulate(match(x,ux)))]
}
myknn <- function(data, x, k)
{
  nb <- neighbors(data, x)
  f <- kclosest(nb, k)
  myknn <- mode(data[f, n])
}
x1 <- c(1.51621, 12.53, 3.48, 1.39, 73.39, 0.60, 8.55, 0.00, 0.05)
x2 <- c(1.5098, 12.77, 1.85, 1.81, 72.69, 0.59, 10.01, 0.00, 0.01)
#recheck max amd min
max(data[,1])
max(data[,2])
min(data[,1])
min(data[,2])
#renormalize for the data 
newdata_n <- data_n
newdata_n[,1] <- (data[,1]-1.5098) / (max(data[,1])-1.5098)
#normalize
normalize <- function(x)
{
  x[1]=(x[1]-1.5098) / (max(data[,1])-1.5098) 
  min = min(data[,2])
  max = max(data[,2])
  x[2] = (x[2] - min) / (max - min)
  for(i in 3:9)
  {
  mean = mean(data[,i])
  sd = sd(data[,i])
  x[i] = (x[i]-mean) / sd
  }
  normalize <- x
}
x1 <- normalize(x1)
x2 <- normalize(x2)
case1 <- myknn(newdata_n, x1, 10)
case2 <- myknn(newdata_n, x2, 10)
case1
case2
```

So the case1 is type 1 case2 is type2.

###Question 8

```{r}
n = ncol(data)
train = training[,1:n-1]
test = testing[,1:n-1]
train_type = training[,n]
test_type = testing[,n]
test_pred <- class::knn(train, test, train_type, 14)
newcase1 <- class::knn(newdata_n[,1:9], x1, newdata_n[,10], 14)
newcase2 <- class::knn(newdata_n[,1:9], x2, newdata_n[,10], 14)
newcase1
newcase2
```

Redo the cases from Qustion 7, I got the same outcome. 

###Question 9

```{r}
confusionMatrix(test_pred, test_type)
```

So the accuracy is 0.6449.

###Question 10

```{r}
#get the knn outcome 
a <- matrix(0, nrow = nrow(test), ncol = 10)
testframe <- data.frame(a)
colnames(testframe) <- c("k=5", "k=6", "k=7", "k=8" ,"k=9", "k=10", "k=11", "k=12", "k=13", "k=14")
myknn(train, test[2,], 5)
for(j in 1:10)
{
  for(i in 1:nrow(test))
  {
    testframe[i,j] <- myknn(training, test[i,], j+4)
  }
}
head(testframe)
```

```{r}
#calcuate the accuracy
accuracy <- numeric(10)
for(i in 1:10)
{
  accuracy[i] <- (sum(testframe[,i] == test_type) / nrow(test)) 
}
maxacc <- order(accuracy)[10]
#the optimal k and the accuracy
colnames(testframe[maxacc])
max(accuracy)
```

###Question 11

```{r}
k <- c(5,6,7,8,9,10,11,12,13,14)
kaccuracy <- data.frame(cbind(k, 1-accuracy))
ggplot(kaccuracy, mapping = aes(x = k, y = accuracy))+
  geom_point()+
  geom_line()
```

###Quesiton 12

```{r}
confusionMatrix(testframe$`k=9`, test_type)
confusionMatrix(test_pred, test_type)
```

###Question 13

The total run time is equal to O(wnm + sk^2 lg(n)). So, as w, n and m increasing, the algorithm behave would be slower. If the training data set and the number of features are large (n and m are large), this algorithm would be slower, too.

##Problem 2

```{r}
library(FNN)
kchouse = read.csv("/Users/xingyang/Desktop/5030/kc_house_data.csv")

#how many case
nrow(kchouse)
#how many features
colnames(kchouse)
#delete id and data
n <- ncol(kchouse)
kchouse <- kchouse[,3:n]
head(kchouse)
#normalize data
kchouse_n <- kchouse
n <- ncol(kchouse)
for(i in 2:n)
{
  min = min(kchouse_n[,i])
  max = max(kchouse_n[,i])
  kchouse_n[,i] = (kchouse_n[,i] - min) / (max - min)
}
kchouse_n$zipcode <- kchouse$zipcode
kchouse_n$lat <- kchouse$lat
kchouse_n$long <- kchouse$long
head(kchouse_n)
```

```{r}
#set training and valiadation data set
set.seed(300)
kcindxTrain <- createDataPartition(y = kchouse_n$price,p = 0.75,list = FALSE)
kctraining <- kchouse_n[kcindxTrain,]
kctesting <- kchouse_n[-kcindxTrain,]
kctrain_price <- kctraining$price
kctest_price <- kctesting$price
kctrain <- kctraining[-1]
kctest <- kctesting[-1]

# I try k from 1 to 50 
library(ModelMetrics)
pred <- as.data.frame(matrix(0, ncol = 50, nrow = nrow(kctest)))
mse <- numeric(50)
for(i in 1:50){
  pred[,i] <- (knn.reg(kctrain, kctest, kctrain_price, i))$pred
  mse[i] = mse(kctest_price, pred[,i])
}
optimal_k <- order(mse)[1]
optimal_k
#so the optimal k is 5

# I choose 3 cases to predict
x1 <- c(4,	3,	2950,	5000,	2,	2,	3,	3,	9,	1980,	970,	1979,	0,	98126,	47.5714,	-122.375,	2140,	4000)
x2 <- c(3,	2.5,	2300,	3060,	1.5,	0,	3,	3,	8,	1510,	790,	1930,	2002,	98115,	47.6827,	-122.31,	1590,	3264)
x3 <- c(3,	1,	1570,	2280,	2,	0,	0,	2,	7,	1570,	0,	1922,	0,	98119,	47.6413,	-122.364,	1580,	2640)
kccase <- data.frame(rbind(x1, x2, x3))
kccase_n <- kccase
m <- ncol(kccase_n)
for(i in 1:m)
{
  min = min(kchouse_n[,i])
  max = max(kchouse_n[,i])
  kccase_n[,i] = (kccase_n[,i] - min) / (max - min)
}
kccase_n[14:16] <- kccase[14:16]
case_pred <- knn.reg(kctrain, kccase_n, kctrain_price, 5)
case_pred

#evaluate the model, V2 is the predictional price 
tp <- data.frame(cbind(kctest_price, pred[,5]))
ggplot(tp, mapping = aes(kctest_price, V2))+
  geom_smooth(method = "gam", se = F, col = "red")+
  geom_point(size=0.1)+
  geom_abline(col = "blue") +
  coord_fixed()
```

Comparing the red line(prediciton) and blue line(observation), my model still have some errors. The prediciton would be lower than the observation but almost match.

##Problem 3

```{r}
rate = read.csv("/Users/xingyang/Desktop/5030/rate.csv")
head(rate)
n <- nrow(rate)
#calcuate the standard error
#samplie moving average
rate$Ft <- 0
rate$E <- 0
rate$Ft[2:n] <- SMA(rate[, 2], n = 3)[1:n-1]
rate$E[4:n] <- abs(rate[4:n,2]-rate$Ft[4:n])
df_sma <- rate
mean(df_sma$E[which(df_sma$E != 0)])

#weighted moving average
rate$Ft <- 0
rate$E <- 0
rate$Ft[2:n] <- WMA(rate[,2], n = 3, wts = c(1, 1, 3))[1:n-1]
rate$E[4:n] = abs(rate[4:n,2] - rate[4:n,3])
df_wam <- rate
mean(df_wam$E[which(df_wam$E != 0)])

#exponential smoothing
rate$Ft <- 0
rate$E <- 0
rate$Ft[1] <- rate[1,2]
rate$Ft[2:n] <- EMA(rate[,2], wilder = F, n = 1, ratio = 0.2)[1:n-1]
rate$E = abs(rate[,2] - rate[,3])
df_ema <- rate
mean(df_ema$E[which(df_ema$E != 0)])

#linear regression trendline
fit <- lm(rate$OccupancyRate~rate$Period)
print(fit)
rate$Ft <- 0
rate$E <- 0
for(i in 1:n){
  rate$Ft[i] <- 34.9419 + 0.0151*rate[i,1]
  rate$E[i] <- abs(rate[i, 2] - rate$Ft[i])
}
df_lr <- rate
mean(df_lr$E)
```

As we can see, the weight moving average has the minimum MAD, so I use weight moving average to forecast.

```{r}
#weighted moving average forecast
fcst_wam <- WMA(rate[,2], n = 3, wts = c(1, 1, 3))[n]
fcst_wam

n <- nrow(df_wam)
#95% Prediction Interval
fcst_wam+c(-1,1) * 1.96 * sd(df_wam[4:n, 3])
```

```{r}
a <- df_wam[4:n, ]
bias <- sum(a[,3]-a[,2])/nrow(a)
bias
```

As we can see, the bias is equal to 1.27 which means the forecast is a litte high.















